{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from imageio import imread\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_glob(rootdir=\".\", suffix=\"\"):\n",
    "    \"\"\"Performs recursive glob with given suffix and rootdir\n",
    "        :param rootdir is the root directory\n",
    "        :param suffix is the suffix to be searched\n",
    "    \"\"\"\n",
    "    return [\n",
    "        os.path.join(looproot, filename)\n",
    "        for looproot, _, filenames in os.walk(rootdir)\n",
    "        for filename in filenames\n",
    "        if filename.endswith(suffix)\n",
    "    ]\n",
    "\n",
    "class CityScapesDataset(Dataset):\n",
    "    colors = [  # [  0,   0,   0],\n",
    "        [128, 64, 128],\n",
    "        [244, 35, 232],\n",
    "        [70, 70, 70],\n",
    "        [102, 102, 156],\n",
    "        [190, 153, 153],\n",
    "        [153, 153, 153],\n",
    "        [250, 170, 30],\n",
    "        [220, 220, 0],\n",
    "        [107, 142, 35],\n",
    "        [152, 251, 152],\n",
    "        [0, 130, 180],\n",
    "        [220, 20, 60],\n",
    "        [255, 0, 0],\n",
    "        [0, 0, 142],\n",
    "        [0, 0, 70],\n",
    "        [0, 60, 100],\n",
    "        [0, 80, 100],\n",
    "        [0, 0, 230],\n",
    "        [119, 11, 32],\n",
    "    ]\n",
    "    \n",
    "    label_colours = dict(zip(range(19), colors))\n",
    "    \n",
    "    def __init__(self, \n",
    "                 root, \n",
    "                 split='train', \n",
    "                 is_transform=False,\n",
    "                 img_size=(256, 256),\n",
    "                 img_norm=True,\n",
    "                 test_mode=False):\n",
    "        self.root = root\n",
    "        self.split = split\n",
    "        self.is_transform = is_transform\n",
    "        self.img_norm = img_norm\n",
    "        self.n_classes = 19\n",
    "        self.img_size = img_size if isinstance(img_size, tuple) else (img_size, img_size)\n",
    "        self.files = {}\n",
    "        \n",
    "        self.images_base = os.path.join(self.root, \"leftImg8bit\", self.split)\n",
    "        self.annotations_base = os.path.join(self.root, \"gtFine\", self.split)\n",
    "        \n",
    "        self.files[split] = recursive_glob(rootdir=self.images_base, suffix=\".png\")\n",
    "\n",
    "        self.void_classes = [0, 1, 2, 3, 4, 5, 6, 9, 10, 14, 15, 16, 18, 29, 30, -1]\n",
    "        self.valid_classes = [\n",
    "            7,\n",
    "            8,\n",
    "            11,\n",
    "            12,\n",
    "            13,\n",
    "            17,\n",
    "            19,\n",
    "            20,\n",
    "            21,\n",
    "            22,\n",
    "            23,\n",
    "            24,\n",
    "            25,\n",
    "            26,\n",
    "            27,\n",
    "            28,\n",
    "            31,\n",
    "            32,\n",
    "            33,\n",
    "        ]\n",
    "        self.class_names = [\n",
    "            \"unlabelled\",\n",
    "            \"road\",\n",
    "            \"sidewalk\",\n",
    "            \"building\",\n",
    "            \"wall\",\n",
    "            \"fence\",\n",
    "            \"pole\",\n",
    "            \"traffic_light\",\n",
    "            \"traffic_sign\",\n",
    "            \"vegetation\",\n",
    "            \"terrain\",\n",
    "            \"sky\",\n",
    "            \"person\",\n",
    "            \"rider\",\n",
    "            \"car\",\n",
    "            \"truck\",\n",
    "            \"bus\",\n",
    "            \"train\",\n",
    "            \"motorcycle\",\n",
    "            \"bicycle\",\n",
    "        ]\n",
    "\n",
    "        self.ignore_index = 250\n",
    "        self.class_map = dict(zip(self.valid_classes, range(19)))\n",
    "        \n",
    "        if not self.files[split]:\n",
    "            raise Exception(\"No files for split=[%s] found in %s\" % (split, self.images_base))\n",
    "\n",
    "        print(\"Found %d %s images\" % (len(self.files[split]), split))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files[self.split])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.files[self.split][index].rstrip()\n",
    "        lbl_path = os.path.join(\n",
    "            self.annotations_base,\n",
    "            img_path.split(os.sep)[-2],\n",
    "            os.path.basename(img_path)[:-15] + \"gtFine_labelIds.png\",\n",
    "        )\n",
    "        \n",
    "        img = imread(img_path)\n",
    "        img = np.array(img, dtype=np.uint8)\n",
    "\n",
    "        lbl = imread(lbl_path)\n",
    "        lbl = self.encode_segmap(np.array(lbl, dtype=np.uint8))\n",
    "        \n",
    "        if self.is_transform:\n",
    "            img, lbl = self.transform(img, lbl)\n",
    "        lbl = lbl.unsqueeze(0)\n",
    "        lbl = lbl.float()\n",
    "        return img, lbl\n",
    "    \n",
    "    def decode_segmap(self, temp):\n",
    "        r = temp.copy()\n",
    "        g = temp.copy()\n",
    "        b = temp.copy()\n",
    "        for l in range(0, self.n_classes):\n",
    "            r[temp == l] = self.label_colours[l][0]\n",
    "            g[temp == l] = self.label_colours[l][1]\n",
    "            b[temp == l] = self.label_colours[l][2]\n",
    "\n",
    "        rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n",
    "        rgb[:, :, 0] = r / 255.0\n",
    "        rgb[:, :, 1] = g / 255.0\n",
    "        rgb[:, :, 2] = b / 255.0\n",
    "        return rgb\n",
    "\n",
    "    def encode_segmap(self, mask):\n",
    "        # Put all void classes to zero\n",
    "        for _voidc in self.void_classes:\n",
    "            mask[mask == _voidc] = self.ignore_index\n",
    "        for _validc in self.valid_classes:\n",
    "            mask[mask == _validc] = self.class_map[_validc]\n",
    "        return mask\n",
    "    \n",
    "    def transform(self, img, lbl):\n",
    "        \"\"\"transform\n",
    "        :param img:\n",
    "        :param lbl:\n",
    "        \"\"\"\n",
    "        img = cv2.resize(img, (self.img_size[0], self.img_size[1]))\n",
    "        # img = m.imresize(img, (self.img_size[0], self.img_size[1]))  # uint8 with RGB mode\n",
    "        img = img[:, :, ::-1]  # RGB -> BGR\n",
    "        img = img.astype(np.float64)\n",
    "        if self.img_norm:\n",
    "            # Resize scales images from 0 to 255, thus we need\n",
    "            # to divide by 255.0\n",
    "            img = img.astype(float) / 255.0\n",
    "        # NHWC -> NCHW\n",
    "        img = img.transpose(2, 0, 1)\n",
    "\n",
    "        classes = np.unique(lbl)\n",
    "        lbl = lbl.astype(float)\n",
    "        # lbl = m.imresize(lbl, (self.img_size[0], self.img_size[1]), \"nearest\", mode=\"F\")\n",
    "        lbl = cv2.resize(lbl, (self.img_size[0], self.img_size[1]), interpolation=cv2.INTER_NEAREST)\n",
    "        lbl = lbl.astype(int)\n",
    "\n",
    "        if not np.all(classes == np.unique(lbl)):\n",
    "            print(\"WARN: resizing labels yielded fewer classes\")\n",
    "\n",
    "        if not np.all(np.unique(lbl[lbl != self.ignore_index]) < self.n_classes):\n",
    "            print(\"after det\", classes, np.unique(lbl))\n",
    "            raise ValueError(\"Segmentation map contained invalid class values\")\n",
    "\n",
    "        img = torch.from_numpy(img).float()\n",
    "        lbl = torch.from_numpy(lbl).long()\n",
    "        \n",
    "        return img, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2975 train images\n",
      "Found 500 val images\n"
     ]
    }
   ],
   "source": [
    "path = '/home/kushaj/Desktop/Data/City Scape Dataset'\n",
    "dataset = {\n",
    "    x: CityScapesDataset(path, split=x, is_transform=True) for x in ['train', 'val']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
